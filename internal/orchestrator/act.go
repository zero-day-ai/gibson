package orchestrator

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"time"

	"github.com/neo4j/neo4j-go-driver/v5/neo4j/dbtype"
	"github.com/zero-day-ai/gibson/internal/agent"
	"github.com/zero-day-ai/gibson/internal/graphrag/graph"
	"github.com/zero-day-ai/gibson/internal/graphrag/queries"
	"github.com/zero-day-ai/gibson/internal/graphrag/schema"
	"github.com/zero-day-ai/gibson/internal/harness"
	"github.com/zero-day-ai/gibson/internal/types"
	"github.com/zero-day-ai/sdk/api/gen/graphragpb"
	"github.com/zero-day-ai/sdk/toolerr"
	"google.golang.org/protobuf/encoding/protojson"
)

// Harness defines the interface for agent delegation and tool execution.
// This matches the SDK harness interface for seamless agent integration.
type Harness interface {
	// DelegateToAgent delegates a task to another agent and waits for the result
	DelegateToAgent(ctx context.Context, agentName string, task agent.Task) (agent.Result, error)
}

// DiscoveryProcessor processes DiscoveryResult from agent outputs and stores to Neo4j.
// This interface is implemented by the graphrag/processor package or an adapter.
type DiscoveryProcessor interface {
	// ProcessAgentDiscovery stores discovered nodes from a proto DiscoveryResult in the graph.
	// This is specifically for processing agent output (not tool output).
	// The missionRunID parameter identifies which MissionRun node the discovered entities should be attached to,
	// enabling mission-scoped data isolation and proper lineage tracking.
	// Returns statistics about nodes/relationships created and any errors.
	ProcessAgentDiscovery(ctx context.Context, missionID, missionRunID, agentName, agentRunID string, discovery *graphragpb.DiscoveryResult) (nodesCreated int, err error)
}

// Actor executes orchestrator decisions by performing the appropriate actions
// in the graph and delegating to agents as needed.
type Actor struct {
	harness            Harness
	execQueries        *queries.ExecutionQueries
	missionQueries     *queries.MissionQueries
	graphClient        graph.GraphClient
	inventory          *ComponentInventory // Component inventory for validation
	missionTracer      interface{}         // *observability.MissionTracer for Langfuse tracing (optional, can be nil)
	policyChecker      PolicyChecker       // Policy checker for data reuse enforcement (optional, can be nil)
	discoveryProcessor DiscoveryProcessor  // Processes DiscoveryResult from agent outputs (optional, can be nil)
	logger             *slog.Logger        // Logger for Actor operations
}

// NewActor creates a new Actor with the given dependencies.
// The harness is used for agent delegation and tool execution.
// The queries provide graph operations for tracking execution state.
// The inventory parameter is optional and used for component validation.
// The missionTracer parameter is optional and enables Langfuse observability when provided.
// The policyChecker parameter is optional and enables data reuse policy enforcement when provided.
// The discoveryProcessor parameter is optional and enables automatic storage of DiscoveryResult from agent outputs.
func NewActor(harness Harness, execQueries *queries.ExecutionQueries, missionQueries *queries.MissionQueries, graphClient graph.GraphClient, inventory *ComponentInventory, missionTracer interface{}, policyChecker PolicyChecker, discoveryProcessor DiscoveryProcessor, logger *slog.Logger) *Actor {
	if logger == nil {
		logger = slog.Default()
	}
	return &Actor{
		harness:            harness,
		execQueries:        execQueries,
		missionQueries:     missionQueries,
		graphClient:        graphClient,
		inventory:          inventory,
		missionTracer:      missionTracer,
		policyChecker:      policyChecker,
		discoveryProcessor: discoveryProcessor,
		logger:             logger.With("component", "actor"),
	}
}

// ActionResult contains the outcome of executing a decision action.
// It includes all relevant state changes, execution results, and metadata
// needed for logging to Langfuse and updating the graph.
type ActionResult struct {
	// Action is the decision action that was executed
	Action DecisionAction

	// AgentExecution contains execution details if an agent was run
	AgentExecution *schema.AgentExecution

	// NewNode contains the newly spawned node if spawn_agent was used
	NewNode *schema.WorkflowNode

	// Error contains any error that occurred during action execution
	Error error

	// IsTerminal indicates if this action ends the orchestration loop
	IsTerminal bool

	// TargetNodeID is the node that was acted upon
	TargetNodeID string

	// Metadata contains additional action-specific metadata
	Metadata map[string]interface{}
}

// Act executes the given decision and returns the result.
// This method orchestrates all the actions needed to fulfill the decision,
// including graph updates, agent delegation, and error handling.
func (a *Actor) Act(ctx context.Context, decision *Decision, missionID string) (*ActionResult, error) {
	if decision == nil {
		return nil, fmt.Errorf("decision cannot be nil")
	}

	// Validate decision before acting
	if err := decision.Validate(); err != nil {
		return nil, fmt.Errorf("invalid decision: %w", err)
	}

	// Parse mission ID
	parsedMissionID, err := types.ParseID(missionID)
	if err != nil {
		return nil, fmt.Errorf("invalid mission ID: %w", err)
	}

	// Route to appropriate action handler
	switch decision.Action {
	case ActionExecuteAgent:
		return a.executeAgent(ctx, decision, parsedMissionID)

	case ActionSkipAgent:
		return a.skipAgent(ctx, decision, parsedMissionID)

	case ActionModifyParams:
		return a.modifyParams(ctx, decision, parsedMissionID)

	case ActionRetry:
		return a.retryAgent(ctx, decision, parsedMissionID)

	case ActionSpawnAgent:
		return a.spawnAgent(ctx, decision, parsedMissionID)

	case ActionComplete:
		return a.completeMission(ctx, decision, parsedMissionID)

	default:
		return nil, fmt.Errorf("unknown decision action: %s", decision.Action)
	}
}

// executeAgent executes the workflow node by delegating to the appropriate agent.
// This creates an execution node, delegates to the agent, and updates the graph
// with the execution results.
func (a *Actor) executeAgent(ctx context.Context, decision *Decision, missionID types.ID) (*ActionResult, error) {
	// Get the workflow node
	node, err := a.getWorkflowNode(ctx, decision.TargetNodeID)
	if err != nil {
		return nil, fmt.Errorf("failed to get workflow node: %w", err)
	}

	// Verify it's an agent node
	if node.Type != schema.WorkflowNodeTypeAgent {
		return nil, fmt.Errorf("node %s is not an agent node (type: %s)", node.ID, node.Type)
	}

	// Check data reuse policy if policy checker is configured
	if a.policyChecker != nil {
		shouldExecute, reason := a.policyChecker.ShouldExecute(ctx, node.AgentName)
		if !shouldExecute {
			// Policy check failed - skip this agent execution
			slog.Info("agent execution skipped by data reuse policy",
				"agent_name", node.AgentName,
				"node_id", node.ID.String(),
				"reason", reason)

			// Mark the node as skipped
			if err := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusSkipped); err != nil {
				return nil, fmt.Errorf("failed to update node status to skipped: %w", err)
			}

			// Return a result indicating the agent was skipped
			return &ActionResult{
				Action:       ActionExecuteAgent,
				Error:        nil,
				IsTerminal:   false,
				TargetNodeID: decision.TargetNodeID,
				Metadata: map[string]interface{}{
					"agent_name":   node.AgentName,
					"skipped":      true,
					"skip_reason":  reason,
					"policy_check": true,
				},
			}, nil
		}
	}

	// Determine attempt number by counting previous executions
	prevExecutions, err := a.execQueries.GetNodeExecutions(ctx, node.ID.String())
	if err != nil {
		return nil, fmt.Errorf("failed to get previous executions: %w", err)
	}
	attemptNum := len(prevExecutions) + 1

	// Create agent execution node
	execution := schema.NewAgentExecution(node.ID.String(), missionID)
	execution.WithAttempt(attemptNum)
	execution.WithConfig(node.TaskConfig)

	// Create execution in graph
	if err := a.execQueries.CreateAgentExecution(ctx, execution); err != nil {
		return nil, fmt.Errorf("failed to create agent execution: %w", err)
	}

	// Create AgentExecutionLog for Langfuse tracing if tracer is available
	// This will be passed to the agent harness to link LLM calls to this execution span
	// We pass it via context so the harness can access it during delegation
	if a.missionTracer != nil {
		// Build the log entry with execution metadata
		// The actual *observability.AgentExecutionLog will be created by the middleware
		// We pass the raw data through context to avoid import cycles
		agentExecData := map[string]interface{}{
			"execution_id":     execution.ID.String(),
			"agent_name":       node.AgentName,
			"workflow_node_id": node.ID.String(),
			"config_used":      node.TaskConfig,
			"attempt":          attemptNum,
		}

		// Store both the tracer and execution data in context for the harness/middleware to access
		ctx = context.WithValue(ctx, "langfuse_mission_tracer", a.missionTracer)
		ctx = context.WithValue(ctx, "langfuse_agent_exec_data", agentExecData)
	}

	// Update workflow node status to running
	if err := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusRunning); err != nil {
		return nil, fmt.Errorf("failed to update node status: %w", err)
	}

	// Build agent task from node's TaskConfig
	// TaskConfig contains: goal, context (merged with task config), input, name, description, etc.
	task := agent.Task{
		ID:        types.NewID(),
		Name:      node.Name,
		MissionID: &missionID,
		CreatedAt: time.Now(),
	}

	// Set timeout
	if node.Timeout > 0 {
		task.Timeout = node.Timeout
	} else {
		task.Timeout = 30 * time.Minute // Default
	}

	// Set description from node or TaskConfig
	task.Description = node.Description
	if desc, ok := node.TaskConfig["description"].(string); ok && desc != "" {
		task.Description = desc
	}

	// Set Goal from TaskConfig (populated by workflow parser from YAML goal field)
	if goal, ok := node.TaskConfig["goal"].(string); ok {
		task.Goal = goal
	} else if task.Description != "" {
		task.Goal = task.Description // Fall back to description
	}

	// Set Context from TaskConfig (contains merged context + task config from YAML)
	if ctx, ok := node.TaskConfig["context"].(map[string]any); ok {
		task.Context = ctx
	} else {
		// If no context, use entire TaskConfig as context for backwards compatibility
		task.Context = node.TaskConfig
	}

	// Keep Input for backwards compatibility (deprecated)
	if input, ok := node.TaskConfig["input"].(map[string]any); ok {
		task.Input = input
	} else {
		task.Input = node.TaskConfig
	}

	// Delegate to agent
	result, err := a.harness.DelegateToAgent(ctx, node.AgentName, task)

	// Update execution based on result
	if err != nil {
		// Extract structured error if available
		var toolErr *toolerr.Error
		if errors.As(err, &toolErr) {
			// Enrich error with registry hints
			toolErr = toolerr.EnrichError(toolErr)

			// Store structured error details
			execution.MarkFailedWithDetails(
				toolErr.Message,
				string(toolErr.Class),
				toolErr.Code,
				toolErr.Hints,
			)
		} else {
			// Fall back to existing behavior for non-toolerr errors
			execution.MarkFailed(err.Error())
		}

		if updateErr := a.execQueries.UpdateExecution(ctx, execution); updateErr != nil {
			return nil, fmt.Errorf("failed to update execution after error: %w", updateErr)
		}

		// Update node status to failed
		if updateErr := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusFailed); updateErr != nil {
			return nil, fmt.Errorf("failed to update node status: %w", updateErr)
		}

		return &ActionResult{
			Action:         ActionExecuteAgent,
			AgentExecution: execution,
			Error:          err,
			IsTerminal:     false,
			TargetNodeID:   decision.TargetNodeID,
		}, nil
	}

	// Check if agent execution failed
	if result.Status == agent.ResultStatusFailed {
		errMsg := "agent execution failed"
		var errCode string

		if result.Error != nil {
			errMsg = result.Error.Message
			errCode = result.Error.Code

			// Log detailed error info at debug level
			slog.Debug("agent execution failed with structured error",
				"agent_name", node.AgentName,
				"error_code", result.Error.Code,
				"error_message", result.Error.Message,
				"recoverable", result.Error.Recoverable,
			)

			// Store structured error details in execution if error code is available
			if errCode != "" {
				// Infer class from code and store structured error details
				errorClass := string(toolerr.DefaultClassForCode(errCode))
				execution.MarkFailedWithDetails(
					errMsg,
					errorClass,
					errCode,
					nil, // No hints available at this level
				)
			} else {
				// No error code, use basic MarkFailed
				execution.MarkFailed(errMsg)
			}
		} else {
			execution.MarkFailed(errMsg)
		}

		// Add error code to metadata if available
		if errCode != "" {
			if execution.Result == nil {
				execution.Result = make(map[string]any)
			}
			execution.Result["error_code"] = errCode
		}

		// Update node status to failed
		if updateErr := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusFailed); updateErr != nil {
			return nil, fmt.Errorf("failed to update node status: %w", updateErr)
		}
	} else {
		// Execution succeeded
		execution.MarkCompleted()
		execution.WithResult(result.Output)

		// Extract missionRunID from context for discovery processing
		missionRunID := harness.MissionRunIDFromContext(ctx)

		// Process DiscoveryResult from agent output if present
		// This stores discovered hosts, ports, services, etc. to Neo4j for use by downstream agents
		a.processAgentDiscovery(ctx, result.Output, node.AgentName, execution.ID.String(), missionID, missionRunID)

		// Update node status to completed
		if updateErr := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusCompleted); updateErr != nil {
			return nil, fmt.Errorf("failed to update node status: %w", updateErr)
		}

		// Update dependent nodes to ready if their dependencies are now satisfied
		if updateErr := a.updateDependentNodesToReady(ctx, node.ID); updateErr != nil {
			slog.Warn("failed to update dependent nodes to ready",
				"node_id", node.ID.String(),
				"error", updateErr,
			)
			// Don't fail the whole operation, just log the warning
		}
	}

	// Update execution in graph
	if updateErr := a.execQueries.UpdateExecution(ctx, execution); updateErr != nil {
		return nil, fmt.Errorf("failed to update execution: %w", updateErr)
	}

	return &ActionResult{
		Action:         ActionExecuteAgent,
		AgentExecution: execution,
		Error:          nil,
		IsTerminal:     false,
		TargetNodeID:   decision.TargetNodeID,
		Metadata: map[string]interface{}{
			"agent_name":     node.AgentName,
			"attempt":        attemptNum,
			"result_status":  string(result.Status),
			"findings_count": len(result.Findings),
		},
	}, nil
}

// skipAgent marks a workflow node as skipped with the reasoning from the decision.
// This is used when the orchestrator determines a node doesn't need to execute.
func (a *Actor) skipAgent(ctx context.Context, decision *Decision, missionID types.ID) (*ActionResult, error) {
	// Get the workflow node
	node, err := a.getWorkflowNode(ctx, decision.TargetNodeID)
	if err != nil {
		return nil, fmt.Errorf("failed to get workflow node: %w", err)
	}

	// Update node status to skipped
	if err := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusSkipped); err != nil {
		return nil, fmt.Errorf("failed to update node status: %w", err)
	}

	return &ActionResult{
		Action:       ActionSkipAgent,
		Error:        nil,
		IsTerminal:   false,
		TargetNodeID: decision.TargetNodeID,
		Metadata: map[string]interface{}{
			"reasoning":  decision.Reasoning,
			"confidence": decision.Confidence,
		},
	}, nil
}

// modifyParams updates the task configuration for a workflow node and then executes it.
// This allows the orchestrator to adapt agent parameters based on context.
func (a *Actor) modifyParams(ctx context.Context, decision *Decision, missionID types.ID) (*ActionResult, error) {
	// Get the workflow node
	node, err := a.getWorkflowNode(ctx, decision.TargetNodeID)
	if err != nil {
		return nil, fmt.Errorf("failed to get workflow node: %w", err)
	}

	// Merge modifications into task config
	mergedConfig := make(map[string]interface{})
	for k, v := range node.TaskConfig {
		mergedConfig[k] = v
	}
	for k, v := range decision.Modifications {
		mergedConfig[k] = v
	}

	// Update node task config in graph
	if err := a.updateNodeConfig(ctx, node.ID, mergedConfig); err != nil {
		return nil, fmt.Errorf("failed to update node config: %w", err)
	}

	// Reload node with updated config
	node.TaskConfig = mergedConfig

	// Now execute the agent with modified params
	// Create a new decision for execute_agent
	execDecision := &Decision{
		Reasoning:    fmt.Sprintf("Executing with modified params: %s", decision.Reasoning),
		Action:       ActionExecuteAgent,
		TargetNodeID: decision.TargetNodeID,
		Confidence:   decision.Confidence,
	}

	return a.executeAgent(ctx, execDecision, missionID)
}

// retryAgent re-executes a failed workflow node, optionally with modified parameters.
// This implements retry logic based on the node's retry policy.
func (a *Actor) retryAgent(ctx context.Context, decision *Decision, missionID types.ID) (*ActionResult, error) {
	// Get the workflow node
	node, err := a.getWorkflowNode(ctx, decision.TargetNodeID)
	if err != nil {
		return nil, fmt.Errorf("failed to get workflow node: %w", err)
	}

	// Check retry policy
	if node.RetryPolicy != nil {
		prevExecutions, err := a.execQueries.GetNodeExecutions(ctx, node.ID.String())
		if err != nil {
			return nil, fmt.Errorf("failed to get previous executions: %w", err)
		}

		if len(prevExecutions) >= node.RetryPolicy.MaxRetries {
			return nil, fmt.Errorf("max retries (%d) exceeded for node %s", node.RetryPolicy.MaxRetries, node.ID)
		}
	}

	// If modifications are provided, apply them first
	if len(decision.Modifications) > 0 {
		mergedConfig := make(map[string]interface{})
		for k, v := range node.TaskConfig {
			mergedConfig[k] = v
		}
		for k, v := range decision.Modifications {
			mergedConfig[k] = v
		}

		if err := a.updateNodeConfig(ctx, node.ID, mergedConfig); err != nil {
			return nil, fmt.Errorf("failed to update node config: %w", err)
		}

		node.TaskConfig = mergedConfig
	}

	// Reset node status to ready for retry
	if err := a.updateNodeStatus(ctx, node.ID, schema.WorkflowNodeStatusReady); err != nil {
		return nil, fmt.Errorf("failed to reset node status: %w", err)
	}

	// Execute the agent
	execDecision := &Decision{
		Reasoning:    fmt.Sprintf("Retry attempt: %s", decision.Reasoning),
		Action:       ActionExecuteAgent,
		TargetNodeID: decision.TargetNodeID,
		Confidence:   decision.Confidence,
	}

	return a.executeAgent(ctx, execDecision, missionID)
}

// spawnAgent creates a new workflow node dynamically and optionally executes it.
// This allows the orchestrator to adapt the workflow at runtime.
func (a *Actor) spawnAgent(ctx context.Context, decision *Decision, missionID types.ID) (*ActionResult, error) {
	if decision.SpawnConfig == nil {
		return nil, fmt.Errorf("spawn_config is required for spawn_agent action")
	}

	// Validate agent exists in inventory before creating node
	if a.inventory != nil {
		validator := NewInventoryValidator(a.inventory)
		if err := validator.ValidateSpawnAgent(decision); err != nil {
			// Return validation error - don't create the workflow node
			return &ActionResult{
				Action:       ActionSpawnAgent,
				Error:        err,
				IsTerminal:   false,
				TargetNodeID: "",
			}, nil // Return nil error - this is a validation failure, not a system error
		}
	}

	cfg := decision.SpawnConfig

	// Create new workflow node
	nodeID := types.NewID()
	node := schema.NewAgentNode(
		nodeID,
		missionID,
		cfg.AgentName,
		cfg.Description,
		cfg.AgentName,
	)
	node.MarkDynamic("orchestrator") // Mark as dynamically spawned
	node.WithTaskConfig(cfg.TaskConfig)
	node.WithStatus(schema.WorkflowNodeStatusReady)

	// Create node in graph
	if err := a.createWorkflowNode(ctx, node); err != nil {
		return nil, fmt.Errorf("failed to create workflow node: %w", err)
	}

	// Create dependencies if specified
	if len(cfg.DependsOn) > 0 {
		if err := a.createNodeDependencies(ctx, nodeID, cfg.DependsOn); err != nil {
			return nil, fmt.Errorf("failed to create node dependencies: %w", err)
		}
	}

	// Link to mission
	if err := a.linkNodeToMission(ctx, missionID, nodeID); err != nil {
		return nil, fmt.Errorf("failed to link node to mission: %w", err)
	}

	return &ActionResult{
		Action:       ActionSpawnAgent,
		NewNode:      node,
		Error:        nil,
		IsTerminal:   false,
		TargetNodeID: nodeID.String(),
		Metadata: map[string]interface{}{
			"agent_name":  cfg.AgentName,
			"description": cfg.Description,
			"depends_on":  cfg.DependsOn,
		},
	}, nil
}

// completeMission marks the mission as complete and returns a terminal result.
// This stops the orchestration loop.
func (a *Actor) completeMission(ctx context.Context, decision *Decision, missionID types.ID) (*ActionResult, error) {
	// Update mission status in graph
	cypher := `
		MATCH (m:Mission {id: $mission_id})
		SET m.status = 'completed',
			m.completed_at = datetime()
		RETURN m.id as id
	`

	params := map[string]interface{}{
		"mission_id": missionID.String(),
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return nil, fmt.Errorf("failed to update mission status: %w", err)
	}

	if len(result.Records) == 0 {
		return nil, fmt.Errorf("mission %s not found", missionID)
	}

	return &ActionResult{
		Action:     ActionComplete,
		Error:      nil,
		IsTerminal: true,
		Metadata: map[string]interface{}{
			"stop_reason": decision.StopReason,
			"confidence":  decision.Confidence,
		},
	}, nil
}

// getWorkflowNode retrieves a workflow node by ID from the graph.
func (a *Actor) getWorkflowNode(ctx context.Context, nodeID string) (*schema.WorkflowNode, error) {
	cypher := `
		MATCH (n:WorkflowNode {id: $node_id})
		RETURN n
	`

	params := map[string]interface{}{
		"node_id": nodeID,
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return nil, fmt.Errorf("failed to query workflow node: %w", err)
	}

	if len(result.Records) == 0 {
		return nil, fmt.Errorf("workflow node %s not found", nodeID)
	}

	nodeData := result.Records[0]["n"]
	if nodeData == nil {
		return nil, fmt.Errorf("workflow node data is nil")
	}

	// Handle different return types from Neo4j driver
	var dataMap map[string]any
	switch n := nodeData.(type) {
	case dbtype.Node:
		dataMap = n.Props
	case map[string]any:
		dataMap = n
	default:
		return nil, fmt.Errorf("invalid node data format: got %T", nodeData)
	}

	return a.parseWorkflowNode(dataMap)
}

// updateNodeStatus updates the status of a workflow node in the graph.
func (a *Actor) updateNodeStatus(ctx context.Context, nodeID types.ID, status schema.WorkflowNodeStatus) error {
	cypher := `
		MATCH (n:WorkflowNode {id: $node_id})
		SET n.status = $status,
			n.updated_at = datetime()
		RETURN n.id as id
	`

	params := map[string]interface{}{
		"node_id": nodeID.String(),
		"status":  string(status),
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return fmt.Errorf("failed to update node status: %w", err)
	}

	if len(result.Records) == 0 {
		return fmt.Errorf("node %s not found", nodeID)
	}

	return nil
}

// updateDependentNodesToReady marks nodes as "ready" if all their dependencies are completed.
// This should be called after a node completes to propagate readiness through the DAG.
func (a *Actor) updateDependentNodesToReady(ctx context.Context, completedNodeID types.ID) error {
	// Find all nodes that depend on the completed node and check if all their dependencies are satisfied
	// A node becomes ready if:
	// 1. It is currently "pending"
	// 2. All nodes it DEPENDS_ON are "completed"
	cypher := `
		MATCH (completed:WorkflowNode {id: $completed_node_id})
		MATCH (dependent:WorkflowNode)-[:DEPENDS_ON]->(completed)
		WHERE dependent.status = 'pending'
		WITH dependent
		OPTIONAL MATCH (dependent)-[:DEPENDS_ON]->(dep:WorkflowNode)
		WITH dependent, collect(dep) as dependencies
		WHERE ALL(d IN dependencies WHERE d.status = 'completed')
		SET dependent.status = 'ready', dependent.updated_at = datetime()
		RETURN dependent.id as id, dependent.name as name
	`

	params := map[string]interface{}{
		"completed_node_id": completedNodeID.String(),
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return fmt.Errorf("failed to update dependent nodes: %w", err)
	}

	// Log which nodes were marked as ready
	for _, record := range result.Records {
		if id, ok := record["id"].(string); ok {
			name := record["name"]
			slog.Info("marked dependent node as ready",
				"node_id", id,
				"node_name", name,
				"triggered_by", completedNodeID.String(),
			)
		}
	}

	return nil
}

// updateNodeConfig updates the task configuration of a workflow node in the graph.
func (a *Actor) updateNodeConfig(ctx context.Context, nodeID types.ID, config map[string]interface{}) error {
	configJSON, err := json.Marshal(config)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	cypher := `
		MATCH (n:WorkflowNode {id: $node_id})
		SET n.task_config = $config,
			n.updated_at = datetime()
		RETURN n.id as id
	`

	params := map[string]interface{}{
		"node_id": nodeID.String(),
		"config":  string(configJSON),
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return fmt.Errorf("failed to update node config: %w", err)
	}

	if len(result.Records) == 0 {
		return fmt.Errorf("node %s not found", nodeID)
	}

	return nil
}

// createWorkflowNode creates a new workflow node in the graph.
func (a *Actor) createWorkflowNode(ctx context.Context, node *schema.WorkflowNode) error {
	if err := node.Validate(); err != nil {
		return fmt.Errorf("invalid workflow node: %w", err)
	}

	configJSON, err := node.TaskConfigJSON()
	if err != nil {
		return fmt.Errorf("failed to marshal task config: %w", err)
	}

	retryPolicyJSON, err := node.RetryPolicyJSON()
	if err != nil {
		return fmt.Errorf("failed to marshal retry policy: %w", err)
	}

	cypher := `
		CREATE (n:WorkflowNode)
		SET n.id = $id,
			n.mission_id = $mission_id,
			n.type = $type,
			n.name = $name,
			n.description = $description,
			n.agent_name = $agent_name,
			n.tool_name = $tool_name,
			n.timeout = $timeout,
			n.retry_policy = $retry_policy,
			n.task_config = $task_config,
			n.status = $status,
			n.is_dynamic = $is_dynamic,
			n.spawned_by = $spawned_by,
			n.created_at = datetime(),
			n.updated_at = datetime()
		RETURN n.id as id
	`

	params := map[string]interface{}{
		"id":           node.ID.String(),
		"mission_id":   node.MissionID.String(),
		"type":         string(node.Type),
		"name":         node.Name,
		"description":  node.Description,
		"agent_name":   node.AgentName,
		"tool_name":    node.ToolName,
		"timeout":      node.Timeout.Milliseconds(),
		"retry_policy": retryPolicyJSON,
		"task_config":  configJSON,
		"status":       string(node.Status),
		"is_dynamic":   node.IsDynamic,
		"spawned_by":   node.SpawnedBy,
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return fmt.Errorf("failed to create workflow node: %w", err)
	}

	if len(result.Records) == 0 {
		return fmt.Errorf("failed to create workflow node")
	}

	return nil
}

// createNodeDependencies creates DEPENDS_ON relationships between nodes.
func (a *Actor) createNodeDependencies(ctx context.Context, nodeID types.ID, dependsOn []string) error {
	if len(dependsOn) == 0 {
		return nil
	}

	cypher := `
		MATCH (n:WorkflowNode {id: $node_id})
		WITH n
		UNWIND $depends_on as dep_id
		MATCH (dep:WorkflowNode {id: dep_id})
		MERGE (n)-[:DEPENDS_ON]->(dep)
		RETURN count(*) as count
	`

	params := map[string]interface{}{
		"node_id":    nodeID.String(),
		"depends_on": dependsOn,
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return fmt.Errorf("failed to create dependencies: %w", err)
	}

	if len(result.Records) == 0 {
		return fmt.Errorf("failed to create dependencies")
	}

	return nil
}

// linkNodeToMission creates a PART_OF relationship from node to mission.
func (a *Actor) linkNodeToMission(ctx context.Context, missionID, nodeID types.ID) error {
	cypher := `
		MATCH (m:Mission {id: $mission_id})
		MATCH (n:WorkflowNode {id: $node_id})
		MERGE (n)-[:PART_OF]->(m)
		RETURN count(*) as count
	`

	params := map[string]interface{}{
		"mission_id": missionID.String(),
		"node_id":    nodeID.String(),
	}

	result, err := a.graphClient.Query(ctx, cypher, params)
	if err != nil {
		return fmt.Errorf("failed to link node to mission: %w", err)
	}

	if len(result.Records) == 0 {
		return fmt.Errorf("failed to link node to mission")
	}

	return nil
}

// parseWorkflowNode converts a Neo4j result map to a WorkflowNode struct.
func (a *Actor) parseWorkflowNode(data map[string]interface{}) (*schema.WorkflowNode, error) {
	id, err := types.ParseID(data["id"].(string))
	if err != nil {
		return nil, fmt.Errorf("invalid node ID: %w", err)
	}

	missionID, err := types.ParseID(data["mission_id"].(string))
	if err != nil {
		return nil, fmt.Errorf("invalid mission ID: %w", err)
	}

	node := &schema.WorkflowNode{
		ID:          id,
		MissionID:   missionID,
		Type:        schema.WorkflowNodeType(data["type"].(string)),
		Name:        data["name"].(string),
		Description: data["description"].(string),
		Status:      schema.WorkflowNodeStatus(data["status"].(string)),
		IsDynamic:   data["is_dynamic"].(bool),
		TaskConfig:  make(map[string]interface{}),
	}

	// Optional fields
	if agentName, ok := data["agent_name"].(string); ok && agentName != "" {
		node.AgentName = agentName
	}
	if toolName, ok := data["tool_name"].(string); ok && toolName != "" {
		node.ToolName = toolName
	}
	if spawnedBy, ok := data["spawned_by"].(string); ok && spawnedBy != "" {
		node.SpawnedBy = spawnedBy
	}
	if timeout, ok := data["timeout"].(int64); ok && timeout > 0 {
		node.Timeout = time.Duration(timeout) * time.Millisecond
	}
	if createdAt, ok := data["created_at"].(time.Time); ok {
		node.CreatedAt = createdAt
	}
	if updatedAt, ok := data["updated_at"].(time.Time); ok {
		node.UpdatedAt = updatedAt
	}

	// Parse JSON fields
	if taskConfigStr, ok := data["task_config"].(string); ok && taskConfigStr != "" && taskConfigStr != "{}" {
		if err := json.Unmarshal([]byte(taskConfigStr), &node.TaskConfig); err != nil {
			return nil, fmt.Errorf("failed to unmarshal task_config: %w", err)
		}
	}

	if retryPolicyStr, ok := data["retry_policy"].(string); ok && retryPolicyStr != "" && retryPolicyStr != "{}" {
		var retryPolicy schema.RetryPolicy
		if err := json.Unmarshal([]byte(retryPolicyStr), &retryPolicy); err != nil {
			return nil, fmt.Errorf("failed to unmarshal retry_policy: %w", err)
		}
		node.RetryPolicy = &retryPolicy
	}

	return node, nil
}

// processAgentDiscovery processes DiscoveryResult from an agent's output and stores it in Neo4j.
// This enables downstream agents to query discovered hosts, ports, services, etc.
//
// The output can be:
// - *graphragpb.DiscoveryResult directly
// - map[string]any (when deserialized from gRPC TypedValue)
//
// This method is non-blocking and errors are logged but not propagated.
func (a *Actor) processAgentDiscovery(ctx context.Context, output any, agentName string, agentRunID string, missionID types.ID, missionRunID string) {
	a.logger.Info("processAgentDiscovery called",
		"agent_name", agentName,
		"has_discovery_processor", a.discoveryProcessor != nil,
		"has_output", output != nil,
		"output_type", fmt.Sprintf("%T", output),
	)

	if a.discoveryProcessor == nil {
		a.logger.Warn("discoveryProcessor is nil, skipping discovery processing")
		return // No processor configured
	}

	if output == nil {
		a.logger.Debug("output is nil, skipping discovery processing")
		return // No output to process
	}

	// Try to extract DiscoveryResult from output
	var discovery *graphragpb.DiscoveryResult

	switch v := output.(type) {
	case *graphragpb.DiscoveryResult:
		discovery = v
		a.logger.Info("received direct DiscoveryResult from agent output",
			"agent_name", agentName,
			"hosts", len(discovery.Hosts),
			"ports", len(discovery.Ports),
		)
	case map[string]any:
		// Output is a map (deserialized from gRPC TypedValue) - try to convert to DiscoveryResult
		// Log the map keys for debugging
		keys := make([]string, 0, len(v))
		for k := range v {
			keys = append(keys, k)
		}
		a.logger.Info("processing agent output map for discovery data",
			"agent_name", agentName,
			"output_keys", keys,
			"output_type", fmt.Sprintf("%T", output),
		)

		// Check if the map looks like a DiscoveryResult by checking for expected fields
		if _, hasHosts := v["hosts"]; hasHosts {
			// Convert map to JSON, then unmarshal into DiscoveryResult proto
			jsonBytes, err := json.Marshal(v)
			if err != nil {
				a.logger.Debug("failed to marshal output map to JSON",
					"error", err,
					"agent_name", agentName,
				)
				return
			}

			discovery = &graphragpb.DiscoveryResult{}
			unmarshaler := protojson.UnmarshalOptions{
				DiscardUnknown: true,
			}
			if err := unmarshaler.Unmarshal(jsonBytes, discovery); err != nil {
				a.logger.Debug("failed to unmarshal output to DiscoveryResult",
					"error", err,
					"agent_name", agentName,
				)
				return
			}

			a.logger.Debug("converted map output to DiscoveryResult",
				"agent_name", agentName,
				"hosts", len(discovery.Hosts),
				"ports", len(discovery.Ports),
			)
		} else if _, hasPorts := v["ports"]; hasPorts {
			// Also check for ports field (some outputs may only have ports)
			jsonBytes, err := json.Marshal(v)
			if err != nil {
				return
			}
			discovery = &graphragpb.DiscoveryResult{}
			unmarshaler := protojson.UnmarshalOptions{DiscardUnknown: true}
			if err := unmarshaler.Unmarshal(jsonBytes, discovery); err != nil {
				return
			}
		} else {
			// Map doesn't look like a DiscoveryResult
			a.logger.Debug("map output does not contain hosts or ports fields, skipping discovery processing",
				"agent_name", agentName,
				"keys", keys,
			)
			return
		}
	default:
		// Output is not a DiscoveryResult or convertible map, nothing to process
		a.logger.Debug("output is not DiscoveryResult or map, skipping discovery processing",
			"agent_name", agentName,
			"output_type", fmt.Sprintf("%T", output),
		)
		return
	}

	if discovery == nil {
		return
	}

	// Check if discovery has any data
	nodeCount := len(discovery.Hosts) + len(discovery.Ports) + len(discovery.Services) +
		len(discovery.Endpoints) + len(discovery.Domains) + len(discovery.Subdomains) +
		len(discovery.Technologies) + len(discovery.Certificates) + len(discovery.Findings) +
		len(discovery.Evidence) + len(discovery.CustomNodes)

	if nodeCount == 0 {
		return // Empty discovery
	}

	// Log warning if missionRunID is not provided
	if missionRunID == "" {
		a.logger.Warn("missionRunID not provided, discovery relationships will not be scoped to mission run",
			"agent_name", agentName,
			"mission_id", missionID.String(),
		)
	}

	a.logger.Info("processing discovery result from agent output",
		"agent_name", agentName,
		"mission_id", missionID.String(),
		"mission_run_id", missionRunID,
		"node_count", nodeCount,
		"hosts", len(discovery.Hosts),
		"ports", len(discovery.Ports),
		"services", len(discovery.Services),
		"endpoints", len(discovery.Endpoints),
	)

	// Process discovery asynchronously with timeout
	go func() {
		processCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
		defer cancel()

		nodesCreated, err := a.discoveryProcessor.ProcessAgentDiscovery(processCtx, missionID.String(), missionRunID, agentName, agentRunID, discovery)
		if err != nil {
			a.logger.Error("failed to process agent discovery result",
				"error", err,
				"agent_name", agentName,
				"mission_id", missionID.String(),
				"mission_run_id", missionRunID,
			)
		} else {
			a.logger.Info("successfully stored agent discovery result",
				"agent_name", agentName,
				"mission_id", missionID.String(),
				"mission_run_id", missionRunID,
				"nodes_created", nodesCreated,
			)
		}
	}()
}
