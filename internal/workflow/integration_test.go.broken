package workflow

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"github.com/zero-day-ai/gibson/internal/agent"
	"github.com/zero-day-ai/gibson/internal/llm"
	"go.opentelemetry.io/otel/trace"
	"go.opentelemetry.io/otel/trace/noop"
	"gopkg.in/yaml.v3"
)

// ============================================================================
// Mock Harness Implementation
// ============================================================================

// mockHarness is a mock implementation of harness.AgentHarness for testing.
// It tracks execution order, simulates various scenarios (success, failure, retries),
// and provides control over execution behavior for comprehensive testing.
type mockHarness struct {
	mu sync.Mutex

	// Execution tracking
	executionOrder []string        // Track order of node executions
	executionTimes map[string]time.Time // Track when each node started

	// Behavior configuration
	failNodes     map[string]int  // Map of node ID -> number of times to fail before succeeding
	failCounts    map[string]int  // Track current failure count for each node
	delayNodes    map[string]time.Duration // Map of node ID -> execution delay
	outputNodes   map[string]map[string]any // Map of node ID -> output to return

	// Agent/Tool/Plugin results
	agentResults  map[string]*agent.Result
	toolResults   map[string]map[string]any
	pluginResults map[string]any

	// Observability
	logger *slog.Logger
	tracer trace.Tracer
}

// newMockHarness creates a new mock harness with default configuration
func newMockHarness() *mockHarness {
	return &mockHarness{
		executionOrder: []string{},
		executionTimes: make(map[string]time.Time),
		failNodes:      make(map[string]int),
		failCounts:     make(map[string]int),
		delayNodes:     make(map[string]time.Duration),
		outputNodes:    make(map[string]map[string]any),
		agentResults:   make(map[string]*agent.Result),
		toolResults:    make(map[string]map[string]any),
		pluginResults:  make(map[string]any),
		logger:         slog.Default(),
		tracer:         noop.NewTracerProvider().Tracer("mock"),
	}
}

// SetNodeFailCount configures a node to fail N times before succeeding
func (m *mockHarness) SetNodeFailCount(nodeID string, count int) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.failNodes[nodeID] = count
}

// SetNodeDelay configures a delay for a specific node
func (m *mockHarness) SetNodeDelay(nodeID string, delay time.Duration) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.delayNodes[nodeID] = delay
}

// SetNodeOutput configures the output for a specific node
func (m *mockHarness) SetNodeOutput(nodeID string, output map[string]any) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.outputNodes[nodeID] = output
}

// SetAgentResult configures the result for an agent node
func (m *mockHarness) SetAgentResult(agentName string, result *agent.Result) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.agentResults[agentName] = result
}

// SetToolResult configures the result for a tool node
func (m *mockHarness) SetToolResult(toolName string, result map[string]any) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.toolResults[toolName] = result
}

// SetPluginResult configures the result for a plugin node
func (m *mockHarness) SetPluginResult(pluginKey string, result any) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.pluginResults[pluginKey] = result
}

// GetExecutionOrder returns the order in which nodes were executed
func (m *mockHarness) GetExecutionOrder() []string {
	m.mu.Lock()
	defer m.mu.Unlock()
	result := make([]string, len(m.executionOrder))
	copy(result, m.executionOrder)
	return result
}

// trackExecution records a node execution
func (m *mockHarness) trackExecution(nodeID string) {
	m.mu.Lock()
	defer m.mu.Unlock()
	m.executionOrder = append(m.executionOrder, nodeID)
	m.executionTimes[nodeID] = time.Now()
}

// shouldFail checks if a node should fail on this execution
func (m *mockHarness) shouldFail(nodeID string) bool {
	m.mu.Lock()
	defer m.mu.Unlock()

	maxFails, exists := m.failNodes[nodeID]
	if !exists {
		return false
	}

	currentFails := m.failCounts[nodeID]
	if currentFails < maxFails {
		m.failCounts[nodeID]++
		return true
	}

	return false
}

// getDelay returns the configured delay for a node
func (m *mockHarness) getDelay(nodeID string) time.Duration {
	m.mu.Lock()
	defer m.mu.Unlock()
	return m.delayNodes[nodeID]
}

// getOutput returns the configured output for a node
func (m *mockHarness) getOutput(nodeID string) map[string]any {
	m.mu.Lock()
	defer m.mu.Unlock()
	return m.outputNodes[nodeID]
}

// Implement harness.AgentHarness interface methods

func (m *mockHarness) DelegateToAgent(ctx context.Context, agentName string, task agent.Task) (*agent.Result, error) {
	m.trackExecution(agentName)

	// Apply delay if configured
	if delay := m.getDelay(agentName); delay > 0 {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-time.After(delay):
		}
	}

	// Check if should fail
	if m.shouldFail(agentName) {
		return &agent.Result{
			Status: agent.ResultStatusFailed,
			Error: &agent.Error{
				Code:    "MOCK_FAILURE",
				Message: fmt.Sprintf("Mock failure for agent %s", agentName),
			},
		}, nil
	}

	// Return configured result or default success
	m.mu.Lock()
	result, exists := m.agentResults[agentName]
	m.mu.Unlock()

	if exists {
		return result, nil
	}

	// Default success result
	output := m.getOutput(agentName)
	if output == nil {
		output = map[string]any{"status": "success"}
	}

	return &agent.Result{
		Status: agent.ResultStatusSuccess,
		Output: output,
	}, nil
}

func (m *mockHarness) CallTool(ctx context.Context, toolName string, input map[string]any) (map[string]any, error) {
	m.trackExecution(toolName)

	// Apply delay if configured
	if delay := m.getDelay(toolName); delay > 0 {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-time.After(delay):
		}
	}

	// Check if should fail
	if m.shouldFail(toolName) {
		return nil, fmt.Errorf("mock failure for tool %s", toolName)
	}

	// Return configured result or default
	m.mu.Lock()
	result, exists := m.toolResults[toolName]
	m.mu.Unlock()

	if exists {
		return result, nil
	}

	output := m.getOutput(toolName)
	if output == nil {
		output = map[string]any{"result": "success"}
	}

	return output, nil
}

func (m *mockHarness) QueryPlugin(ctx context.Context, pluginName, method string, params map[string]any) (any, error) {
	pluginKey := fmt.Sprintf("%s.%s", pluginName, method)
	m.trackExecution(pluginKey)

	// Apply delay if configured
	if delay := m.getDelay(pluginKey); delay > 0 {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-time.After(delay):
		}
	}

	// Check if should fail
	if m.shouldFail(pluginKey) {
		return nil, fmt.Errorf("mock failure for plugin %s", pluginKey)
	}

	// Return configured result or default
	m.mu.Lock()
	result, exists := m.pluginResults[pluginKey]
	m.mu.Unlock()

	if exists {
		return result, nil
	}

	return "success", nil
}

func (m *mockHarness) Logger() *slog.Logger {
	return m.logger
}

func (m *mockHarness) Tracer() trace.Tracer {
	return m.tracer
}

// Stub methods for other harness.AgentHarness interface methods
func (m *mockHarness) Complete(ctx context.Context, slot string, messages []llm.Message, opts ...any) (*llm.CompletionResponse, error) {
	return nil, errors.New("not implemented in mock")
}

func (m *mockHarness) CompleteWithTools(ctx context.Context, slot string, messages []llm.Message, tools []llm.Tool, opts ...any) (*llm.CompletionResponse, error) {
	return nil, errors.New("not implemented in mock")
}

func (m *mockHarness) SubmitFinding(ctx context.Context, finding agent.Finding) error {
	return nil
}

func (m *mockHarness) QueryFindings(ctx context.Context, query agent.FindingQuery) ([]agent.Finding, error) {
	return nil, errors.New("not implemented in mock")
}

// ============================================================================
// Test 1: Full Workflow Lifecycle (Build -> Validate -> Execute -> Complete)
// ============================================================================

func TestIntegration_FullWorkflowLifecycle(t *testing.T) {
	// Build workflow
	workflow, err := NewWorkflow("test-workflow").
		WithDescription("Full lifecycle test workflow").
		AddAgentNode("agent1", "scanner", &agent.Task{
			Description: "Scan for vulnerabilities",
		}).
		AddAgentNode("agent2", "analyzer", &agent.Task{
			Description: "Analyze results",
		}).
		AddAgentNode("agent3", "reporter", &agent.Task{
			Description: "Generate report",
		}).
		WithDependency("agent2", "agent1").
		WithDependency("agent3", "agent2").
		Build()

	require.NoError(t, err, "workflow build should succeed")
	require.NotNil(t, workflow, "workflow should not be nil")

	// Validate workflow structure
	assert.Equal(t, "test-workflow", workflow.Name)
	assert.Len(t, workflow.Nodes, 3)
	assert.Len(t, workflow.EntryPoints, 1)
	assert.Contains(t, workflow.EntryPoints, "agent1")
	assert.Len(t, workflow.ExitPoints, 1)
	assert.Contains(t, workflow.ExitPoints, "agent3")

	// Validate DAG
	validator := NewWorkflowValidator()
	validationResult := validator.Validate(workflow)
	require.True(t, validationResult.IsValid, "workflow should be valid")
	require.Empty(t, validationResult.Errors, "should have no validation errors")

	// Execute workflow
	harness := newMockHarness()
	executor := NewWorkflowExecutor(
		WithMaxParallel(5),
		WithLogger(slog.Default()),
	)

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	result, err := executor.Execute(ctx, workflow, harness)

	// Verify execution results
	require.NoError(t, err, "workflow execution should succeed")
	require.NotNil(t, result, "result should not be nil")
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 3, result.NodesExecuted)
	assert.Equal(t, 0, result.NodesFailed)
	assert.Equal(t, 0, result.NodesSkipped)

	// Verify execution order (must respect dependencies)
	execOrder := harness.GetExecutionOrder()
	require.Len(t, execOrder, 3)
	assert.Equal(t, "scanner", execOrder[0], "agent1 should execute first")
	assert.Equal(t, "analyzer", execOrder[1], "agent2 should execute second")
	assert.Equal(t, "reporter", execOrder[2], "agent3 should execute third")

	// Verify all nodes have results
	assert.Len(t, result.NodeResults, 3)
	for nodeID := range workflow.Nodes {
		assert.Contains(t, result.NodeResults, nodeID)
		assert.Equal(t, NodeStatusCompleted, result.NodeResults[nodeID].Status)
	}
}

// ============================================================================
// Test 2: Parallel Execution with Multiple Ready Nodes
// ============================================================================

func TestIntegration_ParallelExecution(t *testing.T) {
	// Build workflow with parallel branches
	//
	//       start
	//      /  |  \
	//     A   B   C  (parallel)
	//      \  |  /
	//       end
	//
	workflow, err := NewWorkflow("parallel-workflow").
		AddToolNode("start", "init", nil).
		AddToolNode("A", "processA", nil).
		AddToolNode("B", "processB", nil).
		AddToolNode("C", "processC", nil).
		AddToolNode("end", "finalize", nil).
		WithDependency("A", "start").
		WithDependency("B", "start").
		WithDependency("C", "start").
		WithDependency("end", "A", "B", "C").
		Build()

	require.NoError(t, err)

	// Set delays to verify concurrent execution
	harness := newMockHarness()
	harness.SetNodeDelay("processA", 100*time.Millisecond)
	harness.SetNodeDelay("processB", 100*time.Millisecond)
	harness.SetNodeDelay("processC", 100*time.Millisecond)

	executor := NewWorkflowExecutor(WithMaxParallel(10))

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	startTime := time.Now()
	result, err := executor.Execute(ctx, workflow, harness)
	duration := time.Since(startTime)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 5, result.NodesExecuted)

	// Verify parallel execution: should take ~100ms, not 300ms
	// (Allow some overhead for goroutine scheduling)
	assert.Less(t, duration, 250*time.Millisecond, "parallel nodes should execute concurrently")

	// Verify execution order
	execOrder := harness.GetExecutionOrder()
	assert.Equal(t, "init", execOrder[0], "start node should execute first")
	assert.Equal(t, "finalize", execOrder[4], "end node should execute last")

	// A, B, C should all execute after start and before end
	parallelNodes := execOrder[1:4]
	assert.Contains(t, parallelNodes, "processA")
	assert.Contains(t, parallelNodes, "processB")
	assert.Contains(t, parallelNodes, "processC")
}

// ============================================================================
// Test 3: Conditional Branching with True/False Paths
// ============================================================================

func TestIntegration_ConditionalBranching_TruePath(t *testing.T) {
	// Build workflow with condition
	//
	//     start
	//       |
	//   condition
	//     /   \
	//  true   false
	//     \   /
	//      end
	//
	workflow, err := NewWorkflow("conditional-workflow").
		AddToolNode("start", "check", nil).
		AddConditionNode("condition", &NodeCondition{
			Expression:  "nodes.start.output.should_proceed == true",
			TrueBranch:  []string{"true-branch"},
			FalseBranch: []string{"false-branch"},
		}).
		AddToolNode("true-branch", "processTruePath", nil).
		AddToolNode("false-branch", "processFalsePath", nil).
		AddToolNode("end", "finalize", nil).
		WithDependency("condition", "start").
		WithDependency("true-branch", "condition").
		WithDependency("false-branch", "condition").
		WithDependency("end", "true-branch", "false-branch").
		Build()

	require.NoError(t, err)

	// Configure harness to return true from start node
	harness := newMockHarness()
	harness.SetToolResult("check", map[string]any{
		"should_proceed": true,
	})

	executor := NewWorkflowExecutor()

	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)

	// Verify condition node executed and took true branch
	condResult := result.NodeResults["condition"]
	require.NotNil(t, condResult)
	assert.Equal(t, NodeStatusCompleted, condResult.Status)
	assert.Equal(t, true, condResult.Output["condition_result"])

	execOrder := harness.GetExecutionOrder()
	assert.Contains(t, execOrder, "check")
	// Note: Condition nodes don't call harness methods, so they won't appear in execution order
}

func TestIntegration_ConditionalBranching_FalsePath(t *testing.T) {
	workflow, err := NewWorkflow("conditional-workflow-false").
		AddToolNode("start", "check", nil).
		AddConditionNode("condition", &NodeCondition{
			Expression:  "nodes.start.output.should_proceed == true",
			TrueBranch:  []string{"true-branch"},
			FalseBranch: []string{"false-branch"},
		}).
		AddToolNode("true-branch", "processTruePath", nil).
		AddToolNode("false-branch", "processFalsePath", nil).
		WithDependency("condition", "start").
		WithDependency("true-branch", "condition").
		WithDependency("false-branch", "condition").
		Build()

	require.NoError(t, err)

	// Configure harness to return false from start node
	harness := newMockHarness()
	harness.SetToolResult("check", map[string]any{
		"should_proceed": false,
	})

	executor := NewWorkflowExecutor()

	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)

	// Verify condition node took false branch
	condResult := result.NodeResults["condition"]
	require.NotNil(t, condResult)
	assert.Equal(t, false, condResult.Output["condition_result"])
}

// ============================================================================
// Test 4: Retry Policy with Transient Failures
// ============================================================================

func TestIntegration_RetryPolicy_TransientFailures(t *testing.T) {
	// Create workflow with retry policy
	workflow, err := NewWorkflow("retry-workflow").
		AddAgentNode("flaky-agent", "flakyScanner", &agent.Task{
			Description: "Flaky scanner that fails initially",
		}).
		Build()

	require.NoError(t, err)

	// Configure retry policy: fail twice, then succeed
	node := workflow.Nodes["flaky-agent"]
	node.RetryPolicy = &RetryPolicy{
		MaxRetries:      3,
		BackoffStrategy: BackoffConstant,
		InitialDelay:    10 * time.Millisecond,
		MaxDelay:        100 * time.Millisecond,
		Multiplier:      2.0,
	}

	// Configure harness to fail twice, then succeed
	harness := newMockHarness()
	harness.SetNodeFailCount("flakyScanner", 2)

	executor := NewWorkflowExecutor()

	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 1, result.NodesExecuted)

	// Verify retry count
	nodeResult := result.NodeResults["flaky-agent"]
	require.NotNil(t, nodeResult)
	assert.Equal(t, NodeStatusCompleted, nodeResult.Status)
	assert.Equal(t, 2, nodeResult.RetryCount, "should have retried twice")

	// Verify execution order shows multiple attempts
	execOrder := harness.GetExecutionOrder()
	assert.Equal(t, 3, len(execOrder), "should have 3 execution attempts (2 failures + 1 success)")
	for _, name := range execOrder {
		assert.Equal(t, "flakyScanner", name)
	}
}

func TestIntegration_RetryPolicy_ExhaustedRetries(t *testing.T) {
	workflow, err := NewWorkflow("retry-exhausted-workflow").
		AddAgentNode("always-fail", "alwaysFails", &agent.Task{
			Description: "Always fails",
		}).
		Build()

	require.NoError(t, err)

	// Configure retry policy
	node := workflow.Nodes["always-fail"]
	node.RetryPolicy = &RetryPolicy{
		MaxRetries:      2,
		BackoffStrategy: BackoffConstant,
		InitialDelay:    5 * time.Millisecond,
	}

	// Configure harness to always fail
	harness := newMockHarness()
	harness.SetNodeFailCount("alwaysFails", 100) // More than max retries

	executor := NewWorkflowExecutor()

	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	// Workflow should complete but node should be failed
	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 0, result.NodesExecuted)
	assert.Equal(t, 1, result.NodesFailed)

	nodeResult := result.NodeResults["always-fail"]
	require.NotNil(t, nodeResult)
	assert.Equal(t, NodeStatusFailed, nodeResult.Status)
	assert.Equal(t, 2, nodeResult.RetryCount)
}

// ============================================================================
// Test 5: Failure Policy - Failed Nodes Mark Workflow as Failed
// ============================================================================

func TestIntegration_FailurePolicy_NodeFailure(t *testing.T) {
	// Build workflow where middle node will fail
	workflow, err := NewWorkflow("failure-workflow").
		AddToolNode("step1", "init", nil).
		AddToolNode("step2", "process", nil).
		AddToolNode("step3", "finalize", nil).
		WithDependency("step2", "step1").
		WithDependency("step3", "step2").
		Build()

	require.NoError(t, err)

	// Configure step2 to always fail
	harness := newMockHarness()
	harness.SetNodeFailCount("process", 100)

	executor := NewWorkflowExecutor()

	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err) // No error from executor itself
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 1, result.NodesExecuted) // Only step1 succeeded
	assert.Equal(t, 1, result.NodesFailed)   // step2 failed
	assert.Equal(t, 1, result.NodesSkipped)  // step3 skipped (dependency failed)

	// Verify node statuses
	assert.Equal(t, NodeStatusCompleted, result.NodeResults["step1"].Status)
	assert.Equal(t, NodeStatusFailed, result.NodeResults["step2"].Status)
	// step3 won't execute because step2 failed
}

// ============================================================================
// Test 6: Context Cancellation During Execution
// ============================================================================

func TestIntegration_ContextCancellation(t *testing.T) {
	// Build workflow with slow nodes
	workflow, err := NewWorkflow("cancellation-workflow").
		AddToolNode("slow1", "slowProcess1", nil).
		AddToolNode("slow2", "slowProcess2", nil).
		AddToolNode("slow3", "slowProcess3", nil).
		WithDependency("slow2", "slow1").
		WithDependency("slow3", "slow2").
		Build()

	require.NoError(t, err)

	// Configure all nodes to be slow
	harness := newMockHarness()
	harness.SetNodeDelay("slowProcess1", 50*time.Millisecond)
	harness.SetNodeDelay("slowProcess2", 1*time.Second)
	harness.SetNodeDelay("slowProcess3", 1*time.Second)

	executor := NewWorkflowExecutor()

	// Create context that will be cancelled mid-execution
	ctx, cancel := context.WithCancel(context.Background())

	// Cancel after first node completes
	go func() {
		time.Sleep(100 * time.Millisecond)
		cancel()
	}()

	result, err := executor.Execute(ctx, workflow, harness)

	// Should get cancellation error
	require.Error(t, err)
	assert.ErrorIs(t, err, context.Canceled)
	assert.Equal(t, WorkflowStatusCancelled, result.Status)

	// Some nodes may have started/completed before cancellation
	execOrder := harness.GetExecutionOrder()
	assert.LessOrEqual(t, len(execOrder), 3, "not all nodes should execute")
}

// ============================================================================
// Test 7: YAML-Defined Workflow Execution
// ============================================================================

func TestIntegration_YAMLWorkflow(t *testing.T) {
	// Define workflow in YAML
	yamlDef := `
id: "test-yaml-workflow"
name: "YAML Workflow Test"
description: "Workflow defined in YAML"
nodes:
  scan:
    id: scan
    type: tool
    name: scanner
    tool_name: vuln_scanner
    tool_input:
      target: "example.com"
  analyze:
    id: analyze
    type: tool
    name: analyzer
    tool_name: result_analyzer
    dependencies:
      - scan
  report:
    id: report
    type: tool
    name: reporter
    tool_name: report_generator
    dependencies:
      - analyze
`

	// Parse YAML into workflow struct
	var workflowDef struct {
		ID          string `yaml:"id"`
		Name        string `yaml:"name"`
		Description string `yaml:"description"`
		Nodes       map[string]struct {
			ID           string         `yaml:"id"`
			Type         string         `yaml:"type"`
			Name         string         `yaml:"name"`
			ToolName     string         `yaml:"tool_name"`
			ToolInput    map[string]any `yaml:"tool_input"`
			Dependencies []string       `yaml:"dependencies"`
		} `yaml:"nodes"`
	}

	err := yaml.Unmarshal([]byte(yamlDef), &workflowDef)
	require.NoError(t, err)

	// Build workflow from YAML
	builder := NewWorkflow(workflowDef.Name).
		WithDescription(workflowDef.Description)

	for _, nodeDef := range workflowDef.Nodes {
		builder.AddToolNode(nodeDef.ID, nodeDef.ToolName, nodeDef.ToolInput)
		if len(nodeDef.Dependencies) > 0 {
			builder.WithDependency(nodeDef.ID, nodeDef.Dependencies...)
		}
	}

	workflow, err := builder.Build()
	require.NoError(t, err)
	assert.Equal(t, "YAML Workflow Test", workflow.Name)
	assert.Len(t, workflow.Nodes, 3)

	// Execute workflow
	harness := newMockHarness()
	harness.SetToolResult("vuln_scanner", map[string]any{
		"vulnerabilities_found": 5,
	})
	harness.SetToolResult("result_analyzer", map[string]any{
		"critical": 2,
		"high":     3,
	})
	harness.SetToolResult("report_generator", map[string]any{
		"report_id": "RPT-001",
	})

	executor := NewWorkflowExecutor()
	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 3, result.NodesExecuted)

	// Verify execution order
	execOrder := harness.GetExecutionOrder()
	assert.Equal(t, []string{"vuln_scanner", "result_analyzer", "report_generator"}, execOrder)
}

// ============================================================================
// Test 8: Complex DAG with Diamond Dependencies
// ============================================================================

func TestIntegration_DiamondDependencies(t *testing.T) {
	// Build diamond DAG:
	//
	//       A
	//      / \
	//     B   C
	//      \ /
	//       D
	//
	workflow, err := NewWorkflow("diamond-workflow").
		AddToolNode("A", "start", nil).
		AddToolNode("B", "pathB", nil).
		AddToolNode("C", "pathC", nil).
		AddToolNode("D", "end", nil).
		WithDependency("B", "A").
		WithDependency("C", "A").
		WithDependency("D", "B", "C").
		Build()

	require.NoError(t, err)

	// Verify DAG structure
	assert.Len(t, workflow.Nodes, 4)
	assert.Equal(t, []string{"A"}, workflow.EntryPoints)
	assert.Equal(t, []string{"D"}, workflow.ExitPoints)

	// Validate no cycles
	validator := NewWorkflowValidator()
	validationResult := validator.Validate(workflow)
	require.True(t, validationResult.IsValid)

	// Execute workflow
	harness := newMockHarness()
	harness.SetNodeDelay("pathB", 50*time.Millisecond)
	harness.SetNodeDelay("pathC", 50*time.Millisecond)

	executor := NewWorkflowExecutor()
	ctx := context.Background()
	startTime := time.Now()
	result, err := executor.Execute(ctx, workflow, harness)
	duration := time.Since(startTime)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 4, result.NodesExecuted)

	// Verify execution order respects dependencies
	execOrder := harness.GetExecutionOrder()
	assert.Equal(t, "start", execOrder[0], "A must execute first")
	assert.Equal(t, "end", execOrder[3], "D must execute last")

	// B and C should execute in parallel after A
	parallelNodes := execOrder[1:3]
	assert.Contains(t, parallelNodes, "pathB")
	assert.Contains(t, parallelNodes, "pathC")

	// Should take ~50ms (parallel B/C), not 100ms (sequential)
	assert.Less(t, duration, 150*time.Millisecond, "B and C should run in parallel")
}

// ============================================================================
// Test 9: Join Node Aggregating Results from Multiple Predecessors
// ============================================================================

func TestIntegration_JoinNodeAggregation(t *testing.T) {
	// Build workflow with join node
	//
	//     A   B   C
	//      \  |  /
	//       join
	//
	workflow, err := NewWorkflow("join-workflow").
		AddToolNode("A", "sourceA", nil).
		AddToolNode("B", "sourceB", nil).
		AddToolNode("C", "sourceC", nil).
		AddNode(&WorkflowNode{
			ID:           "join",
			Type:         NodeTypeJoin,
			Name:         "aggregator",
			Dependencies: []string{"A", "B", "C"},
		}).
		Build()

	require.NoError(t, err)

	// Configure different outputs for each source
	harness := newMockHarness()
	harness.SetToolResult("sourceA", map[string]any{
		"data": "from A",
		"count": 10,
	})
	harness.SetToolResult("sourceB", map[string]any{
		"data": "from B",
		"count": 20,
	})
	harness.SetToolResult("sourceC", map[string]any{
		"data": "from C",
		"count": 30,
	})

	executor := NewWorkflowExecutor()
	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 4, result.NodesExecuted)

	// Verify join node aggregated all results
	joinResult := result.NodeResults["join"]
	require.NotNil(t, joinResult)
	assert.Equal(t, NodeStatusCompleted, joinResult.Status)

	aggregatedOutput, ok := joinResult.Output.(map[string]any)
	require.True(t, ok, "join output should be a map")

	// Verify all three sources are in the aggregated output
	assert.Contains(t, aggregatedOutput, "A")
	assert.Contains(t, aggregatedOutput, "B")
	assert.Contains(t, aggregatedOutput, "C")

	// Verify metadata contains aggregated results
	metadata, ok := joinResult.Metadata["aggregated_results"].(map[string]*NodeResult)
	require.True(t, ok)
	assert.Len(t, metadata, 3)
}

func TestIntegration_JoinNodeWithFailedDependency(t *testing.T) {
	workflow, err := NewWorkflow("join-failure-workflow").
		AddToolNode("A", "sourceA", nil).
		AddToolNode("B", "sourceB", nil).
		AddNode(&WorkflowNode{
			ID:           "join",
			Type:         NodeTypeJoin,
			Name:         "aggregator",
			Dependencies: []string{"A", "B"},
		}).
		Build()

	require.NoError(t, err)

	// Configure sourceB to fail
	harness := newMockHarness()
	harness.SetNodeFailCount("sourceB", 100)

	executor := NewWorkflowExecutor()
	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)

	// Join node should execute but report failure
	joinResult := result.NodeResults["join"]
	require.NotNil(t, joinResult)
	assert.Equal(t, NodeStatusFailed, joinResult.Status)
	assert.NotNil(t, joinResult.Error)
	assert.Contains(t, joinResult.Error.Message, "dependencies failed")
}

// ============================================================================
// Test 10: Parallel Node Executing Sub-Nodes Concurrently
// ============================================================================

func TestIntegration_ParallelNodeSubExecution(t *testing.T) {
	// Create parallel node with sub-nodes
	subNodeA := &WorkflowNode{
		ID:       "subA",
		Type:     NodeTypeTool,
		Name:     "subToolA",
		ToolName: "subToolA",
	}
	subNodeB := &WorkflowNode{
		ID:       "subB",
		Type:     NodeTypeTool,
		Name:     "subToolB",
		ToolName: "subToolB",
	}
	subNodeC := &WorkflowNode{
		ID:       "subC",
		Type:     NodeTypeTool,
		Name:     "subToolC",
		ToolName: "subToolC",
	}

	workflow, err := NewWorkflow("parallel-node-workflow").
		AddNode(&WorkflowNode{
			ID:       "parallel",
			Type:     NodeTypeParallel,
			Name:     "parallel-processor",
			SubNodes: []*WorkflowNode{subNodeA, subNodeB, subNodeC},
		}).
		Build()

	require.NoError(t, err)

	// Configure delays for sub-nodes
	harness := newMockHarness()
	harness.SetNodeDelay("subToolA", 100*time.Millisecond)
	harness.SetNodeDelay("subToolB", 100*time.Millisecond)
	harness.SetNodeDelay("subToolC", 100*time.Millisecond)

	executor := NewWorkflowExecutor()
	ctx := context.Background()

	startTime := time.Now()
	result, err := executor.Execute(ctx, workflow, harness)
	duration := time.Since(startTime)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)

	// Verify parallel node completed
	parallelResult := result.NodeResults["parallel"]
	require.NotNil(t, parallelResult)
	assert.Equal(t, NodeStatusCompleted, parallelResult.Status)

	// Verify sub-results
	subResults, ok := parallelResult.Output["sub_results"].(map[string]*NodeResult)
	require.True(t, ok)
	assert.Len(t, subResults, 3)
	assert.Contains(t, subResults, "subA")
	assert.Contains(t, subResults, "subB")
	assert.Contains(t, subResults, "subC")

	// Verify concurrent execution (should take ~100ms, not 300ms)
	assert.Less(t, duration, 250*time.Millisecond, "sub-nodes should execute concurrently")

	// Verify all sub-nodes were executed
	execOrder := harness.GetExecutionOrder()
	assert.Contains(t, execOrder, "subToolA")
	assert.Contains(t, execOrder, "subToolB")
	assert.Contains(t, execOrder, "subToolC")
}

func TestIntegration_ParallelNodeWithFailedSubNode(t *testing.T) {
	subNodeA := &WorkflowNode{
		ID:       "subA",
		Type:     NodeTypeTool,
		Name:     "subToolA",
		ToolName: "subToolA",
	}
	subNodeB := &WorkflowNode{
		ID:       "subB",
		Type:     NodeTypeTool,
		Name:     "subToolB",
		ToolName: "subToolB",
	}

	workflow, err := NewWorkflow("parallel-node-failure-workflow").
		AddNode(&WorkflowNode{
			ID:       "parallel",
			Type:     NodeTypeParallel,
			Name:     "parallel-processor",
			SubNodes: []*WorkflowNode{subNodeA, subNodeB},
		}).
		Build()

	require.NoError(t, err)

	// Configure subToolB to fail
	harness := newMockHarness()
	harness.SetNodeFailCount("subToolB", 100)

	executor := NewWorkflowExecutor()
	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)

	// Parallel node should report failure
	parallelResult := result.NodeResults["parallel"]
	require.NotNil(t, parallelResult)
	assert.Equal(t, NodeStatusFailed, parallelResult.Status)
	assert.NotNil(t, parallelResult.Error)
	assert.Contains(t, parallelResult.Error.Message, "sub-nodes failed")

	// Verify metadata shows which sub-node failed
	failedNodes, ok := parallelResult.Error.Details["failed_nodes"].([]string)
	require.True(t, ok)
	assert.Contains(t, failedNodes, "subB")
}

// ============================================================================
// Test 11: Exponential Backoff Retry Strategy
// ============================================================================

func TestIntegration_RetryPolicy_ExponentialBackoff(t *testing.T) {
	workflow, err := NewWorkflow("exponential-backoff-workflow").
		AddAgentNode("backoff-agent", "backoffAgent", &agent.Task{
			Description: "Agent with exponential backoff",
		}).
		Build()

	require.NoError(t, err)

	// Configure exponential backoff retry policy
	node := workflow.Nodes["backoff-agent"]
	node.RetryPolicy = &RetryPolicy{
		MaxRetries:      3,
		BackoffStrategy: BackoffExponential,
		InitialDelay:    10 * time.Millisecond,
		MaxDelay:        200 * time.Millisecond,
		Multiplier:      2.0,
	}

	// Configure to fail twice, succeed on third try
	harness := newMockHarness()
	harness.SetNodeFailCount("backoffAgent", 2)

	executor := NewWorkflowExecutor()
	ctx := context.Background()

	startTime := time.Now()
	result, err := executor.Execute(ctx, workflow, harness)
	duration := time.Since(startTime)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)

	nodeResult := result.NodeResults["backoff-agent"]
	require.NotNil(t, nodeResult)
	assert.Equal(t, 2, nodeResult.RetryCount)

	// Verify exponential backoff timing
	// First attempt: immediate
	// First retry: wait 10ms
	// Second retry: wait 20ms
	// Total delay: ~30ms
	assert.GreaterOrEqual(t, duration, 30*time.Millisecond)
	assert.Less(t, duration, 100*time.Millisecond) // Allow overhead
}

// ============================================================================
// Test 12: Max Parallel Limit Enforcement
// ============================================================================

func TestIntegration_MaxParallelLimitEnforcement(t *testing.T) {
	// Create workflow with many parallel nodes
	builder := NewWorkflow("max-parallel-workflow")
	for i := 0; i < 10; i++ {
		builder.AddToolNode(fmt.Sprintf("node%d", i), fmt.Sprintf("tool%d", i), nil)
	}

	workflow, err := builder.Build()
	require.NoError(t, err)

	// Configure all nodes with 100ms delay
	harness := newMockHarness()
	for i := 0; i < 10; i++ {
		harness.SetNodeDelay(fmt.Sprintf("tool%d", i), 100*time.Millisecond)
	}

	// Limit to 3 parallel executions
	executor := NewWorkflowExecutor(WithMaxParallel(3))

	ctx := context.Background()
	startTime := time.Now()
	result, err := executor.Execute(ctx, workflow, harness)
	duration := time.Since(startTime)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Equal(t, 10, result.NodesExecuted)

	// With 10 nodes, 100ms each, limited to 3 parallel:
	// Batch 1: nodes 0-2 (100ms)
	// Batch 2: nodes 3-5 (100ms)
	// Batch 3: nodes 6-8 (100ms)
	// Batch 4: node 9 (100ms)
	// Total: ~400ms
	assert.GreaterOrEqual(t, duration, 300*time.Millisecond)
	assert.Less(t, duration, 600*time.Millisecond)
}

// ============================================================================
// Test 13: Complex Multi-Stage Workflow
// ============================================================================

func TestIntegration_ComplexMultiStageWorkflow(t *testing.T) {
	// Build a complex workflow with multiple stages:
	// 1. Initial scan (parallel)
	// 2. Conditional analysis based on scan results
	// 3. Parallel exploitation attempts
	// 4. Join and report
	workflow, err := NewWorkflow("complex-workflow").
		// Stage 1: Parallel scanning
		AddToolNode("port-scan", "portScanner", nil).
		AddToolNode("vuln-scan", "vulnScanner", nil).
		AddToolNode("web-scan", "webScanner", nil).
		// Stage 2: Condition based on scan results
		AddConditionNode("check-vulns", &NodeCondition{
			Expression:  "nodes.vuln-scan.output.vulnerabilities_found > 0",
			TrueBranch:  []string{"analyze"},
			FalseBranch: []string{"skip"},
		}).
		AddToolNode("analyze", "analyzer", nil).
		AddToolNode("skip", "skipAnalysis", nil).
		// Stage 3: Join scan results
		AddNode(&WorkflowNode{
			ID:           "join-scans",
			Type:         NodeTypeJoin,
			Name:         "join-scanner-results",
			Dependencies: []string{"port-scan", "vuln-scan", "web-scan"},
		}).
		// Stage 4: Final report
		AddToolNode("report", "reportGenerator", nil).
		WithDependency("check-vulns", "vuln-scan").
		WithDependency("analyze", "check-vulns").
		WithDependency("skip", "check-vulns").
		WithDependency("report", "join-scans", "analyze", "skip").
		Build()

	require.NoError(t, err)

	// Configure harness
	harness := newMockHarness()
	harness.SetToolResult("portScanner", map[string]any{"open_ports": []int{22, 80, 443}})
	harness.SetToolResult("vulnScanner", map[string]any{"vulnerabilities_found": 5})
	harness.SetToolResult("webScanner", map[string]any{"endpoints": []string{"/api", "/admin"}})
	harness.SetToolResult("analyzer", map[string]any{"severity": "high"})
	harness.SetToolResult("reportGenerator", map[string]any{"report_id": "RPT-123"})

	executor := NewWorkflowExecutor()
	ctx := context.Background()
	result, err := executor.Execute(ctx, workflow, harness)

	require.NoError(t, err)
	assert.Equal(t, WorkflowStatusCompleted, result.Status)
	assert.Greater(t, result.NodesExecuted, 5)

	// Verify scan nodes executed in parallel
	execOrder := harness.GetExecutionOrder()
	scanNodes := []string{"portScanner", "vulnScanner", "webScanner"}
	for _, scan := range scanNodes {
		assert.Contains(t, execOrder, scan)
	}

	// Verify report was generated last
	assert.Equal(t, "reportGenerator", execOrder[len(execOrder)-1])
}
